{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Naming: \\\\\n",
        "Article 1 - “Distributed Representations of Words and Phrases and their\n",
        "Compositionality” - lab1.pdf \\\\\n",
        "Article 2 -  “Attention is All You Need” - lab1_2.pdf"
      ],
      "metadata": {
        "id": "uj7Uf4YYJLHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Word-based tokenization with RegEx"
      ],
      "metadata": {
        "id": "A_KupUeh2cQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJqkzjZQxdsL",
        "outputId": "ba75dcef-937c-40f9-ce9b-d268fe522032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.24.5)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.3 in /usr/local/lib/python3.10/dist-packages (from pymupdf) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: DLS chat ideas \\\\\n",
        "Ligatures, hyphens in the end of the line and [?] signs for non-unicode symbols handling via pymupdf \\\\"
      ],
      "metadata": {
        "id": "eGyvubmO2jCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymupdf\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    texts = []\n",
        "    flags = 0 | pymupdf.TEXT_DEHYPHENATE | pymupdf.TEXT_CID_FOR_UNKNOWN_UNICODE\n",
        "    with pymupdf.open(pdf_path) as doc:\n",
        "      for page in doc:\n",
        "          text = page.get_text(\"text\", flags=flags)\n",
        "          texts.append(text)\n",
        "      result_text = chr(12).join(texts)\n",
        "\n",
        "    return result_text\n",
        "\n",
        "pdf_path = \"lab1.pdf\"\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "# print(pdf_text)"
      ],
      "metadata": {
        "id": "t5q0EV3rxaYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation, words with intentional hyphens and spaces handling via regex \\\\\n",
        "Remove empty string, numbers and turn to lowercase"
      ],
      "metadata": {
        "id": "VcWyrD8h7zy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r\"[^\\w-]+\"\n",
        "text = re.split(pattern, pdf_text)\n",
        "text = [word for word in text if word]\n",
        "text = [word for word in text if not re.match(r'^\\d+$', word)]\n",
        "text = [word.lower() for word in text]\n",
        "print(len(text))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjzasvETfS_z",
        "outputId": "083eb0a8-ca06-4d0f-ec5b-9f9021b29edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4495\n",
            "['arxiv', '4546v1', 'cs', 'cl', 'oct', 'distributed', 'representations', 'of', 'words', 'and', 'phrases', 'and', 'their', 'compositionality', 'tomas', 'mikolov', 'google', 'inc', 'mountain', 'view', 'mikolov', 'google', 'com', 'ilya', 'sutskever', 'google', 'inc', 'mountain', 'view', 'ilyasu', 'google', 'com', 'kai', 'chen', 'google', 'inc', 'mountain', 'view', 'kai', 'google', 'com', 'greg', 'corrado', 'google', 'inc', 'mountain', 'view', 'gcorrado', 'google', 'com', 'jeffrey', 'dean', 'google', 'inc', 'mountain', 'view', 'jeff', 'google', 'com', 'abstract', 'the', 'recently', 'introduced', 'continuous', 'skip-gram', 'model', 'is', 'an', 'efficient', 'method', 'for', 'learning', 'high-quality', 'distributed', 'vector', 'representations', 'that', 'capture', 'a', 'large', 'number', 'of', 'precise', 'syntactic', 'and', 'semantic', 'word', 'relationships', 'in', 'this', 'paper', 'we', 'present', 'several', 'extensions', 'that', 'improve', 'both', 'the', 'quality', 'of', 'the', 'vectors', 'and', 'the', 'training', 'speed', 'by', 'subsampling', 'of', 'the', 'frequent', 'words', 'we', 'obtain', 'significant', 'speedup', 'and', 'also', 'learn', 'more', 'regular', 'word', 'representations', 'we', 'also', 'describe', 'a', 'simple', 'alternative', 'to', 'the', 'hierarchical', 'softmax', 'called', 'negative', 'sampling', 'an', 'inherent', 'limitation', 'of', 'word', 'representations', 'is', 'their', 'indifference', 'to', 'word', 'order', 'and', 'their', 'inability', 'to', 'represent', 'idiomatic', 'phrases', 'for', 'example', 'the', 'meanings', 'of', 'canada', 'and', 'air', 'cannot', 'be', 'easily', 'combined', 'to', 'obtain', 'air', 'canada', 'motivated', 'by', 'this', 'example', 'we', 'present', 'a', 'simple', 'method', 'for', 'finding', 'phrases', 'in', 'text', 'and', 'show', 'that', 'learning', 'good', 'vector', 'representations', 'for', 'millions', 'of', 'phrases', 'is', 'possible', 'introduction', 'distributed', 'representations', 'of', 'words', 'in', 'a', 'vector', 'space', 'help', 'learning', 'algorithms', 'to', 'achieve', 'better', 'performance', 'in', 'natural', 'language', 'processing', 'tasks', 'by', 'grouping', 'similar', 'words', 'one', 'of', 'the', 'earliest', 'use', 'of', 'word', 'representations', 'dates', 'back', 'to', 'due', 'to', 'rumelhart', 'hinton', 'and', 'williams', 'this', 'idea', 'has', 'since', 'been', 'applied', 'to', 'statistical', 'language', 'modeling', 'with', 'considerable', 'success', 'the', 'follow', 'up', 'work', 'includes', 'applications', 'to', 'automatic', 'speech', 'recognition', 'and', 'machine', 'translation', 'and', 'a', 'wide', 'range', 'of', 'nlp', 'tasks', 'recently', 'mikolov', 'et', 'al', 'introduced', 'the', 'skip-gram', 'model', 'an', 'efficient', 'method', 'for', 'learning', 'highquality', 'vector', 'representations', 'of', 'words', 'from', 'large', 'amounts', 'of', 'unstructured', 'text', 'data', 'unlike', 'most', 'of', 'the', 'previously', 'used', 'neural', 'network', 'architectures', 'for', 'learning', 'word', 'vectors', 'training', 'of', 'the', 'skipgram', 'model', 'see', 'figure', 'does', 'not', 'involve', 'dense', 'matrix', 'multiplications', 'this', 'makes', 'the', 'training', 'extremely', 'efficient', 'an', 'optimized', 'single-machine', 'implementation', 'can', 'train', 'on', 'more', 'than', 'billion', 'words', 'in', 'one', 'day', 'the', 'word', 'representations', 'computed', 'using', 'neural', 'networks', 'are', 'very', 'interesting', 'because', 'the', 'learned', 'vectors', 'explicitly', 'encode', 'many', 'linguistic', 'regularities', 'and', 'patterns', 'somewhat', 'surprisingly', 'many', 'of', 'these', 'patterns', 'can', 'be', 'represented', 'as', 'linear', 'translations', 'for', 'example', 'the', 'result', 'of', 'a', 'vector', 'calculation', 'vec', 'madrid', '-', 'vec', 'spain', 'vec', 'france', 'is', 'closer', 'to', 'vec', 'paris', 'than', 'to', 'any', 'other', 'word', 'vector', 'figure', 'the', 'skip-gram', 'model', 'architecture', 'the', 'training', 'objective', 'is', 'to', 'learn', 'word', 'vector', 'representations', 'that', 'are', 'good', 'at', 'predicting', 'the', 'nearby', 'words', 'in', 'this', 'paper', 'we', 'present', 'several', 'extensions', 'of', 'the', 'original', 'skip-gram', 'model', 'we', 'show', 'that', 'subsampling', 'of', 'frequent', 'words', 'during', 'training', 'results', 'in', 'a', 'significant', 'speedup', 'around', '2x', '-', '10x', 'and', 'improves', 'accuracy', 'of', 'the', 'representations', 'of', 'less', 'frequent', 'words', 'in', 'addition', 'we', 'present', 'a', 'simplified', 'variant', 'of', 'noise', 'contrastive', 'estimation', 'nce', 'for', 'training', 'the', 'skip-gram', 'model', 'that', 'results', 'in', 'faster', 'training', 'and', 'better', 'vector', 'representations', 'for', 'frequent', 'words', 'compared', 'to', 'more', 'complex', 'hierarchical', 'softmax', 'that', 'was', 'used', 'in', 'the', 'prior', 'work', 'word', 'representations', 'are', 'limited', 'by', 'their', 'inability', 'to', 'represent', 'idiomatic', 'phrases', 'that', 'are', 'not', 'compositions', 'of', 'the', 'individual', 'words', 'for', 'example', 'boston', 'globe', 'is', 'a', 'newspaper', 'and', 'so', 'it', 'is', 'not', 'a', 'natural', 'combination', 'of', 'the', 'meanings', 'of', 'boston', 'and', 'globe', 'therefore', 'using', 'vectors', 'to', 'represent', 'the', 'whole', 'phrases', 'makes', 'the', 'skip-gram', 'model', 'considerably', 'more', 'expressive', 'other', 'techniques', 'that', 'aim', 'to', 'represent', 'meaning', 'of', 'sentences', 'by', 'composing', 'the', 'word', 'vectors', 'such', 'as', 'the', 'recursive', 'autoencoders', 'would', 'also', 'benefit', 'from', 'using', 'phrase', 'vectors', 'instead', 'of', 'the', 'word', 'vectors', 'the', 'extension', 'from', 'word', 'based', 'to', 'phrase', 'based', 'models', 'is', 'relatively', 'simple', 'first', 'we', 'identify', 'a', 'large', 'number', 'of', 'phrases', 'using', 'a', 'data-driven', 'approach', 'and', 'then', 'we', 'treat', 'the', 'phrases', 'as', 'individual', 'tokens', 'during', 'the', 'training', 'to', 'evaluate', 'the', 'quality', 'of', 'the', 'phrase', 'vectors', 'we', 'developed', 'a', 'test', 'set', 'of', 'analogical', 'reasoning', 'tasks', 'that', 'contains', 'both', 'words', 'and', 'phrases', 'a', 'typical', 'analogy', 'pair', 'from', 'our', 'test', 'set', 'is', 'montreal', 'montreal', 'canadiens', 'toronto', 'toronto', 'maple', 'leafs', 'it', 'is', 'considered', 'to', 'have', 'been', 'answered', 'correctly', 'if', 'the', 'nearest', 'representation', 'to', 'vec', 'montreal', 'canadiens', '-', 'vec', 'montreal', 'vec', 'toronto', 'is', 'vec', 'toronto', 'maple', 'leafs', 'finally', 'we', 'describe', 'another', 'interesting', 'property', 'of', 'the', 'skip-gram', 'model', 'we', 'found', 'that', 'simple', 'vector', 'addition', 'can', 'often', 'produce', 'meaningful', 'results', 'for', 'example', 'vec', 'russia', 'vec', 'river', 'is', 'close', 'to', 'vec', 'volga', 'river', 'and', 'vec', 'germany', 'vec', 'capital', 'is', 'close', 'to', 'vec', 'berlin', 'this', 'compositionality', 'suggests', 'that', 'a', 'non-obvious', 'degree', 'of', 'language', 'understanding', 'can', 'be', 'obtained', 'by', 'using', 'basic', 'mathematical', 'operations', 'on', 'the', 'word', 'vector', 'representations', 'the', 'skip-gram', 'model', 'the', 'training', 'objective', 'of', 'the', 'skip-gram', 'model', 'is', 'to', 'find', 'word', 'representations', 'that', 'are', 'useful', 'for', 'predicting', 'the', 'surrounding', 'words', 'in', 'a', 'sentence', 'or', 'a', 'document', 'more', 'formally', 'given', 'a', 'sequence', 'of', 'training', 'words', 'w1', 'w2', 'w3', 'wt', 'the', 'objective', 'of', 'the', 'skip-gram', 'model', 'is', 'to', 'maximize', 'the', 'average', 'log', 'probability', 't', 't', 'x', 't', 'x', 'c', 'j', 'c', 'j', 'log', 'p', 'wt', 'j', 'wt', 'where', 'c', 'is', 'the', 'size', 'of', 'the', 'training', 'context', 'which', 'can', 'be', 'a', 'function', 'of', 'the', 'center', 'word', 'wt', 'larger', 'c', 'results', 'in', 'more', 'training', 'examples', 'and', 'thus', 'can', 'lead', 'to', 'a', 'higher', 'accuracy', 'at', 'the', 'expense', 'of', 'the', 'training', 'time', 'the', 'basic', 'skip-gram', 'formulation', 'defines', 'p', 'wt', 'j', 'wt', 'using', 'the', 'softmax', 'function', 'p', 'wo', 'wi', 'exp', 'v', 'wo', 'vwi', 'pw', 'w', 'exp', 'v', 'w', 'vwi', 'where', 'vw', 'and', 'v', 'w', 'are', 'the', 'input', 'and', 'output', 'vector', 'representations', 'of', 'w', 'and', 'w', 'is', 'the', 'number', 'of', 'words', 'in', 'the', 'vocabulary', 'this', 'formulation', 'is', 'impractical', 'because', 'the', 'cost', 'of', 'computing', 'log', 'p', 'wo', 'wi', 'is', 'proportional', 'to', 'w', 'which', 'is', 'often', 'large', 'terms', 'hierarchical', 'softmax', 'a', 'computationally', 'efficient', 'approximation', 'of', 'the', 'full', 'softmax', 'is', 'the', 'hierarchical', 'softmax', 'in', 'the', 'context', 'of', 'neural', 'network', 'language', 'models', 'it', 'was', 'first', 'introduced', 'by', 'morin', 'and', 'bengio', 'the', 'main', 'advantage', 'is', 'that', 'instead', 'of', 'evaluating', 'w', 'output', 'nodes', 'in', 'the', 'neural', 'network', 'to', 'obtain', 'the', 'probability', 'distribution', 'it', 'is', 'needed', 'to', 'evaluate', 'only', 'about', 'log2', 'w', 'nodes', 'the', 'hierarchical', 'softmax', 'uses', 'a', 'binary', 'tree', 'representation', 'of', 'the', 'output', 'layer', 'with', 'the', 'w', 'words', 'as', 'its', 'leaves', 'and', 'for', 'each', 'node', 'explicitly', 'represents', 'the', 'relative', 'probabilities', 'of', 'its', 'child', 'nodes', 'these', 'define', 'a', 'random', 'walk', 'that', 'assigns', 'probabilities', 'to', 'words', 'more', 'precisely', 'each', 'word', 'w', 'can', 'be', 'reached', 'by', 'an', 'appropriate', 'path', 'from', 'the', 'root', 'of', 'the', 'tree', 'let', 'n', 'w', 'j', 'be', 'the', 'j-th', 'node', 'on', 'the', 'path', 'from', 'the', 'root', 'to', 'w', 'and', 'let', 'l', 'w', 'be', 'the', 'length', 'of', 'this', 'path', 'so', 'n', 'w', 'root', 'and', 'n', 'w', 'l', 'w', 'w', 'in', 'addition', 'for', 'any', 'inner', 'node', 'n', 'let', 'ch', 'n', 'be', 'an', 'arbitrary', 'fixed', 'child', 'of', 'n', 'and', 'let', 'x', 'be', 'if', 'x', 'is', 'true', 'and', '-1', 'otherwise', 'then', 'the', 'hierarchical', 'softmax', 'defines', 'p', 'wo', 'wi', 'as', 'follows', 'p', 'w', 'wi', 'l', 'w', 'y', 'j', 'σ', 'n', 'w', 'j', 'ch', 'n', 'w', 'j', 'v', 'n', 'w', 'j', 'vwi', 'where', 'σ', 'x', 'exp', 'x', 'it', 'can', 'be', 'verified', 'that', 'pw', 'w', 'p', 'w', 'wi', 'this', 'implies', 'that', 'the', 'cost', 'of', 'computing', 'log', 'p', 'wo', 'wi', 'and', 'log', 'p', 'wo', 'wi', 'is', 'proportional', 'to', 'l', 'wo', 'which', 'on', 'average', 'is', 'no', 'greater', 'than', 'log', 'w', 'also', 'unlike', 'the', 'standard', 'softmax', 'formulation', 'of', 'the', 'skip-gram', 'which', 'assigns', 'two', 'representations', 'vw', 'and', 'v', 'w', 'to', 'each', 'word', 'w', 'the', 'hierarchical', 'softmax', 'formulation', 'has', 'one', 'representation', 'vw', 'for', 'each', 'word', 'w', 'and', 'one', 'representation', 'v', 'n', 'for', 'every', 'inner', 'node', 'n', 'of', 'the', 'binary', 'tree', 'the', 'structure', 'of', 'the', 'tree', 'used', 'by', 'the', 'hierarchical', 'softmax', 'has', 'a', 'considerable', 'effect', 'on', 'the', 'performance', 'mnih', 'and', 'hinton', 'explored', 'a', 'number', 'of', 'methods', 'for', 'constructing', 'the', 'tree', 'structure', 'and', 'the', 'effect', 'on', 'both', 'the', 'training', 'time', 'and', 'the', 'resulting', 'model', 'accuracy', 'in', 'our', 'work', 'we', 'use', 'a', 'binary', 'huffman', 'tree', 'as', 'it', 'assigns', 'short', 'codes', 'to', 'the', 'frequent', 'words', 'which', 'results', 'in', 'fast', 'training', 'it', 'has', 'been', 'observed', 'before', 'that', 'grouping', 'words', 'together', 'by', 'their', 'frequency', 'works', 'well', 'as', 'a', 'very', 'simple', 'speedup', 'technique', 'for', 'the', 'neural', 'network', 'based', 'language', 'models', 'negative', 'sampling', 'an', 'alternative', 'to', 'the', 'hierarchical', 'softmax', 'is', 'noise', 'contrastive', 'estimation', 'nce', 'which', 'was', 'introduced', 'by', 'gutmann', 'and', 'hyvarinen', 'and', 'applied', 'to', 'language', 'modeling', 'by', 'mnih', 'and', 'teh', 'nce', 'posits', 'that', 'a', 'good', 'model', 'should', 'be', 'able', 'to', 'differentiate', 'data', 'from', 'noise', 'by', 'means', 'of', 'logistic', 'regression', 'this', 'is', 'similar', 'to', 'hinge', 'loss', 'used', 'by', 'collobert', 'and', 'weston', 'who', 'trained', 'the', 'models', 'by', 'ranking', 'the', 'data', 'above', 'noise', 'while', 'nce', 'can', 'be', 'shown', 'to', 'approximately', 'maximize', 'the', 'log', 'probability', 'of', 'the', 'softmax', 'the', 'skipgram', 'model', 'is', 'only', 'concerned', 'with', 'learning', 'high-quality', 'vector', 'representations', 'so', 'we', 'are', 'free', 'to', 'simplify', 'nce', 'as', 'long', 'as', 'the', 'vector', 'representations', 'retain', 'their', 'quality', 'we', 'define', 'negative', 'sampling', 'neg', 'by', 'the', 'objective', 'log', 'σ', 'v', 'wo', 'vwi', 'k', 'x', 'i', 'ewi', 'pn', 'w', 'h', 'log', 'σ', 'v', 'wi', 'vwi', 'i', '-2', '-1', '-1', '-0', '-2', '-1', '-1', '-0', 'country', 'and', 'capital', 'vectors', 'projected', 'by', 'pca', 'china', 'japan', 'france', 'russia', 'germany', 'italy', 'spain', 'greece', 'turkey', 'beijing', 'paris', 'tokyo', 'poland', 'moscow', 'portugal', 'berlin', 'rome', 'athens', 'madrid', 'ankara', 'warsaw', 'lisbon', 'figure', 'two-dimensional', 'pca', 'projection', 'of', 'the', '1000-dimensional', 'skip-gram', 'vectors', 'of', 'countries', 'and', 'their', 'capital', 'cities', 'the', 'figure', 'illustrates', 'ability', 'of', 'the', 'model', 'to', 'automatically', 'organize', 'concepts', 'and', 'learn', 'implicitly', 'the', 'relationships', 'between', 'them', 'as', 'during', 'the', 'training', 'we', 'did', 'not', 'provide', 'any', 'supervised', 'information', 'about', 'what', 'a', 'capital', 'city', 'means', 'which', 'is', 'used', 'to', 'replace', 'every', 'log', 'p', 'wo', 'wi', 'term', 'in', 'the', 'skip-gram', 'objective', 'thus', 'the', 'task', 'is', 'to', 'distinguish', 'the', 'target', 'word', 'wo', 'from', 'draws', 'from', 'the', 'noise', 'distribution', 'pn', 'w', 'using', 'logistic', 'regression', 'where', 'there', 'are', 'k', 'negative', 'samples', 'for', 'each', 'data', 'sample', 'our', 'experiments', 'indicate', 'that', 'values', 'of', 'k', 'in', 'the', 'range', 'are', 'useful', 'for', 'small', 'training', 'datasets', 'while', 'for', 'large', 'datasets', 'the', 'k', 'can', 'be', 'as', 'small', 'as', 'the', 'main', 'difference', 'between', 'the', 'negative', 'sampling', 'and', 'nce', 'is', 'that', 'nce', 'needs', 'both', 'samples', 'and', 'the', 'numerical', 'probabilities', 'of', 'the', 'noise', 'distribution', 'while', 'negative', 'sampling', 'uses', 'only', 'samples', 'and', 'while', 'nce', 'approximately', 'maximizes', 'the', 'log', 'probability', 'of', 'the', 'softmax', 'this', 'property', 'is', 'not', 'important', 'for', 'our', 'application', 'both', 'nce', 'and', 'neg', 'have', 'the', 'noise', 'distribution', 'pn', 'w', 'as', 'a', 'free', 'parameter', 'we', 'investigated', 'a', 'number', 'of', 'choices', 'for', 'pn', 'w', 'and', 'found', 'that', 'the', 'unigram', 'distribution', 'u', 'w', 'raised', 'to', 'the', '4rd', 'power', 'i', 'e', 'u', 'w', 'z', 'outperformed', 'significantly', 'the', 'unigram', 'and', 'the', 'uniform', 'distributions', 'for', 'both', 'nce', 'and', 'neg', 'on', 'every', 'task', 'we', 'tried', 'including', 'language', 'modeling', 'not', 'reported', 'here', 'subsampling', 'of', 'frequent', 'words', 'in', 'very', 'large', 'corpora', 'the', 'most', 'frequent', 'words', 'can', 'easily', 'occur', 'hundreds', 'of', 'millions', 'of', 'times', 'e', 'g', 'in', 'the', 'and', 'a', 'such', 'words', 'usually', 'provide', 'less', 'information', 'value', 'than', 'the', 'rare', 'words', 'for', 'example', 'while', 'the', 'skip-gram', 'model', 'benefits', 'from', 'observing', 'the', 'co-occurrences', 'of', 'france', 'and', 'paris', 'it', 'benefits', 'much', 'less', 'from', 'observing', 'the', 'frequent', 'co-occurrences', 'of', 'france', 'and', 'the', 'as', 'nearly', 'every', 'word', 'co-occurs', 'frequently', 'within', 'a', 'sentence', 'with', 'the', 'this', 'idea', 'can', 'also', 'be', 'applied', 'in', 'the', 'opposite', 'direction', 'the', 'vector', 'representations', 'of', 'frequent', 'words', 'do', 'not', 'change', 'significantly', 'after', 'training', 'on', 'several', 'million', 'examples', 'to', 'counter', 'the', 'imbalance', 'between', 'the', 'rare', 'and', 'frequent', 'words', 'we', 'used', 'a', 'simple', 'subsampling', 'approach', 'each', 'word', 'wi', 'in', 'the', 'training', 'set', 'is', 'discarded', 'with', 'probability', 'computed', 'by', 'the', 'formula', 'p', 'wi', 's', 't', 'f', 'wi', 'method', 'time', 'min', 'syntactic', 'semantic', 'total', 'accuracy', 'neg-5', 'neg-15', 'hs-huffman', 'nce-5', 'the', 'following', 'results', 'use', 'subsampling', 'neg-5', 'neg-15', 'hs-huffman', 'table', 'accuracy', 'of', 'various', 'skip-gram', '300-dimensional', 'models', 'on', 'the', 'analogical', 'reasoning', 'task', 'as', 'defined', 'in', 'neg-k', 'stands', 'for', 'negative', 'sampling', 'with', 'k', 'negative', 'samples', 'for', 'each', 'positive', 'sample', 'nce', 'stands', 'for', 'noise', 'contrastive', 'estimation', 'and', 'hs-huffman', 'stands', 'for', 'the', 'hierarchical', 'softmax', 'with', 'the', 'frequency-based', 'huffman', 'codes', 'where', 'f', 'wi', 'is', 'the', 'frequency', 'of', 'word', 'wi', 'and', 't', 'is', 'a', 'chosen', 'threshold', 'typically', 'around', 'we', 'chose', 'this', 'subsampling', 'formula', 'because', 'it', 'aggressively', 'subsamples', 'words', 'whose', 'frequency', 'is', 'greater', 'than', 't', 'while', 'preserving', 'the', 'ranking', 'of', 'the', 'frequencies', 'although', 'this', 'subsampling', 'formula', 'was', 'chosen', 'heuristically', 'we', 'found', 'it', 'to', 'work', 'well', 'in', 'practice', 'it', 'accelerates', 'learning', 'and', 'even', 'significantly', 'improves', 'the', 'accuracy', 'of', 'the', 'learned', 'vectors', 'of', 'the', 'rare', 'words', 'as', 'will', 'be', 'shown', 'in', 'the', 'following', 'sections', 'empirical', 'results', 'in', 'this', 'section', 'we', 'evaluate', 'the', 'hierarchical', 'softmax', 'hs', 'noise', 'contrastive', 'estimation', 'negative', 'sampling', 'and', 'subsampling', 'of', 'the', 'training', 'words', 'we', 'used', 'the', 'analogical', 'reasoning', 'task1', 'introduced', 'by', 'mikolov', 'et', 'al', 'the', 'task', 'consists', 'of', 'analogies', 'such', 'as', 'germany', 'berlin', 'france', 'which', 'are', 'solved', 'by', 'finding', 'a', 'vector', 'x', 'such', 'that', 'vec', 'x', 'is', 'closest', 'to', 'vec', 'berlin', '-', 'vec', 'germany', 'vec', 'france', 'according', 'to', 'the', 'cosine', 'distance', 'we', 'discard', 'the', 'input', 'words', 'from', 'the', 'search', 'this', 'specific', 'example', 'is', 'considered', 'to', 'have', 'been', 'answered', 'correctly', 'if', 'x', 'is', 'paris', 'the', 'task', 'has', 'two', 'broad', 'categories', 'the', 'syntactic', 'analogies', 'such', 'as', 'quick', 'quickly', 'slow', 'slowly', 'and', 'the', 'semantic', 'analogies', 'such', 'as', 'the', 'country', 'to', 'capital', 'city', 'relationship', 'for', 'training', 'the', 'skip-gram', 'models', 'we', 'have', 'used', 'a', 'large', 'dataset', 'consisting', 'of', 'various', 'news', 'articles', 'an', 'internal', 'google', 'dataset', 'with', 'one', 'billion', 'words', 'we', 'discarded', 'from', 'the', 'vocabulary', 'all', 'words', 'that', 'occurred', 'less', 'than', 'times', 'in', 'the', 'training', 'data', 'which', 'resulted', 'in', 'a', 'vocabulary', 'of', 'size', '692k', 'the', 'performance', 'of', 'various', 'skip-gram', 'models', 'on', 'the', 'word', 'analogy', 'test', 'set', 'is', 'reported', 'in', 'table', 'the', 'table', 'shows', 'that', 'negative', 'sampling', 'outperforms', 'the', 'hierarchical', 'softmax', 'on', 'the', 'analogical', 'reasoning', 'task', 'and', 'has', 'even', 'slightly', 'better', 'performance', 'than', 'the', 'noise', 'contrastive', 'estimation', 'the', 'subsampling', 'of', 'the', 'frequent', 'words', 'improves', 'the', 'training', 'speed', 'several', 'times', 'and', 'makes', 'the', 'word', 'representations', 'significantly', 'more', 'accurate', 'it', 'can', 'be', 'argued', 'that', 'the', 'linearity', 'of', 'the', 'skip-gram', 'model', 'makes', 'its', 'vectors', 'more', 'suitable', 'for', 'such', 'linear', 'analogical', 'reasoning', 'but', 'the', 'results', 'of', 'mikolov', 'et', 'al', 'also', 'show', 'that', 'the', 'vectors', 'learned', 'by', 'the', 'standard', 'sigmoidal', 'recurrent', 'neural', 'networks', 'which', 'are', 'highly', 'non-linear', 'improve', 'on', 'this', 'task', 'significantly', 'as', 'the', 'amount', 'of', 'the', 'training', 'data', 'increases', 'suggesting', 'that', 'non-linear', 'models', 'also', 'have', 'a', 'preference', 'for', 'a', 'linear', 'structure', 'of', 'the', 'word', 'representations', 'learning', 'phrases', 'as', 'discussed', 'earlier', 'many', 'phrases', 'have', 'a', 'meaning', 'that', 'is', 'not', 'a', 'simple', 'composition', 'of', 'the', 'meanings', 'of', 'its', 'individual', 'words', 'to', 'learn', 'vector', 'representation', 'for', 'phrases', 'we', 'first', 'find', 'words', 'that', 'appear', 'frequently', 'together', 'and', 'infrequently', 'in', 'other', 'contexts', 'for', 'example', 'new', 'york', 'times', 'and', 'toronto', 'maple', 'leafs', 'are', 'replaced', 'by', 'unique', 'tokens', 'in', 'the', 'training', 'data', 'while', 'a', 'bigram', 'this', 'is', 'will', 'remain', 'unchanged', '1code', 'google', 'com', 'p', 'word2vec', 'source', 'browse', 'trunk', 'questions-words', 'txt', 'newspapers', 'new', 'york', 'new', 'york', 'times', 'baltimore', 'baltimore', 'sun', 'san', 'jose', 'san', 'jose', 'mercury', 'news', 'cincinnati', 'cincinnati', 'enquirer', 'nhl', 'teams', 'boston', 'boston', 'bruins', 'montreal', 'montreal', 'canadiens', 'phoenix', 'phoenix', 'coyotes', 'nashville', 'nashville', 'predators', 'nba', 'teams', 'detroit', 'detroit', 'pistons', 'toronto', 'toronto', 'raptors', 'oakland', 'golden', 'state', 'warriors', 'memphis', 'memphis', 'grizzlies', 'airlines', 'austria', 'austrian', 'airlines', 'spain', 'spainair', 'belgium', 'brussels', 'airlines', 'greece', 'aegean', 'airlines', 'company', 'executives', 'steve', 'ballmer', 'microsoft', 'larry', 'page', 'google', 'samuel', 'j', 'palmisano', 'ibm', 'werner', 'vogels', 'amazon', 'table', 'examples', 'of', 'the', 'analogical', 'reasoning', 'task', 'for', 'phrases', 'the', 'full', 'test', 'set', 'has', 'examples', 'the', 'goal', 'is', 'to', 'compute', 'the', 'fourth', 'phrase', 'using', 'the', 'first', 'three', 'our', 'best', 'model', 'achieved', 'an', 'accuracy', 'of', 'on', 'this', 'dataset', 'this', 'way', 'we', 'can', 'form', 'many', 'reasonable', 'phrases', 'without', 'greatly', 'increasing', 'the', 'size', 'of', 'the', 'vocabulary', 'in', 'theory', 'we', 'can', 'train', 'the', 'skip-gram', 'model', 'using', 'all', 'n-grams', 'but', 'that', 'would', 'be', 'too', 'memory', 'intensive', 'many', 'techniques', 'have', 'been', 'previously', 'developed', 'to', 'identify', 'phrases', 'in', 'the', 'text', 'however', 'it', 'is', 'out', 'of', 'scope', 'of', 'our', 'work', 'to', 'compare', 'them', 'we', 'decided', 'to', 'use', 'a', 'simple', 'data-driven', 'approach', 'where', 'phrases', 'are', 'formed', 'based', 'on', 'the', 'unigram', 'and', 'bigram', 'counts', 'using', 'score', 'wi', 'wj', 'count', 'wiwj', 'δ', 'count', 'wi', 'count', 'wj', 'the', 'δ', 'is', 'used', 'as', 'a', 'discounting', 'coefficient', 'and', 'prevents', 'too', 'many', 'phrases', 'consisting', 'of', 'very', 'infrequent', 'words', 'to', 'be', 'formed', 'the', 'bigrams', 'with', 'score', 'above', 'the', 'chosen', 'threshold', 'are', 'then', 'used', 'as', 'phrases', 'typically', 'we', 'run', '2-4', 'passes', 'over', 'the', 'training', 'data', 'with', 'decreasing', 'threshold', 'value', 'allowing', 'longer', 'phrases', 'that', 'consists', 'of', 'several', 'words', 'to', 'be', 'formed', 'we', 'evaluate', 'the', 'quality', 'of', 'the', 'phrase', 'representations', 'using', 'a', 'new', 'analogical', 'reasoning', 'task', 'that', 'involves', 'phrases', 'table', 'shows', 'examples', 'of', 'the', 'five', 'categories', 'of', 'analogies', 'used', 'in', 'this', 'task', 'this', 'dataset', 'is', 'publicly', 'available', 'on', 'the', 'web2', 'phrase', 'skip-gram', 'results', 'starting', 'with', 'the', 'same', 'news', 'data', 'as', 'in', 'the', 'previous', 'experiments', 'we', 'first', 'constructed', 'the', 'phrase', 'based', 'training', 'corpus', 'and', 'then', 'we', 'trained', 'several', 'skip-gram', 'models', 'using', 'different', 'hyperparameters', 'as', 'before', 'we', 'used', 'vector', 'dimensionality', 'and', 'context', 'size', 'this', 'setting', 'already', 'achieves', 'good', 'performance', 'on', 'the', 'phrase', 'dataset', 'and', 'allowed', 'us', 'to', 'quickly', 'compare', 'the', 'negative', 'sampling', 'and', 'the', 'hierarchical', 'softmax', 'both', 'with', 'and', 'without', 'subsampling', 'of', 'the', 'frequent', 'tokens', 'the', 'results', 'are', 'summarized', 'in', 'table', 'the', 'results', 'show', 'that', 'while', 'negative', 'sampling', 'achieves', 'a', 'respectable', 'accuracy', 'even', 'with', 'k', 'using', 'k', 'achieves', 'considerably', 'better', 'performance', 'surprisingly', 'while', 'we', 'found', 'the', 'hierarchical', 'softmax', 'to', 'achieve', 'lower', 'performance', 'when', 'trained', 'without', 'subsampling', 'it', 'became', 'the', 'best', 'performing', 'method', 'when', 'we', 'downsampled', 'the', 'frequent', 'words', 'this', 'shows', 'that', 'the', 'subsampling', 'can', 'result', 'in', 'faster', 'training', 'and', 'can', 'also', 'improve', 'accuracy', 'at', 'least', 'in', 'some', 'cases', '2code', 'google', 'com', 'p', 'word2vec', 'source', 'browse', 'trunk', 'questions-phrases', 'txt', 'method', 'dimensionality', 'no', 'subsampling', 'subsampling', 'neg-5', 'neg-15', 'hs-huffman', 'table', 'accuracies', 'of', 'the', 'skip-gram', 'models', 'on', 'the', 'phrase', 'analogy', 'dataset', 'the', 'models', 'were', 'trained', 'on', 'approximately', 'one', 'billion', 'words', 'from', 'the', 'news', 'dataset', 'neg-15', 'with', 'subsampling', 'hs', 'with', 'subsampling', 'vasco', 'de', 'gama', 'lingsugur', 'italian', 'explorer', 'lake', 'baikal', 'great', 'rift', 'valley', 'aral', 'sea', 'alan', 'bean', 'rebbeca', 'naomi', 'moonwalker', 'ionian', 'sea', 'ruegen', 'ionian', 'islands', 'chess', 'master', 'chess', 'grandmaster', 'garry', 'kasparov', 'table', 'examples', 'of', 'the', 'closest', 'entities', 'to', 'the', 'given', 'short', 'phrases', 'using', 'two', 'different', 'models', 'czech', 'currency', 'vietnam', 'capital', 'german', 'airlines', 'russian', 'river', 'french', 'actress', 'koruna', 'hanoi', 'airline', 'lufthansa', 'moscow', 'juliette', 'binoche', 'check', 'crown', 'ho', 'chi', 'minh', 'city', 'carrier', 'lufthansa', 'volga', 'river', 'vanessa', 'paradis', 'polish', 'zolty', 'viet', 'nam', 'flag', 'carrier', 'lufthansa', 'upriver', 'charlotte', 'gainsbourg', 'ctk', 'vietnamese', 'lufthansa', 'russia', 'cecile', 'de', 'table', 'vector', 'compositionality', 'using', 'element-wise', 'addition', 'four', 'closest', 'tokens', 'to', 'the', 'sum', 'of', 'two', 'vectors', 'are', 'shown', 'using', 'the', 'best', 'skip-gram', 'model', 'to', 'maximize', 'the', 'accuracy', 'on', 'the', 'phrase', 'analogy', 'task', 'we', 'increased', 'the', 'amount', 'of', 'the', 'training', 'data', 'by', 'using', 'a', 'dataset', 'with', 'about', 'billion', 'words', 'we', 'used', 'the', 'hierarchical', 'softmax', 'dimensionality', 'of', 'and', 'the', 'entire', 'sentence', 'for', 'the', 'context', 'this', 'resulted', 'in', 'a', 'model', 'that', 'reached', 'an', 'accuracy', 'of', 'we', 'achieved', 'lower', 'accuracy', 'when', 'we', 'reduced', 'the', 'size', 'of', 'the', 'training', 'dataset', 'to', '6b', 'words', 'which', 'suggests', 'that', 'the', 'large', 'amount', 'of', 'the', 'training', 'data', 'is', 'crucial', 'to', 'gain', 'further', 'insight', 'into', 'how', 'different', 'the', 'representations', 'learned', 'by', 'different', 'models', 'are', 'we', 'did', 'inspect', 'manually', 'the', 'nearest', 'neighbours', 'of', 'infrequent', 'phrases', 'using', 'various', 'models', 'in', 'table', 'we', 'show', 'a', 'sample', 'of', 'such', 'comparison', 'consistently', 'with', 'the', 'previous', 'results', 'it', 'seems', 'that', 'the', 'best', 'representations', 'of', 'phrases', 'are', 'learned', 'by', 'a', 'model', 'with', 'the', 'hierarchical', 'softmax', 'and', 'subsampling', 'additive', 'compositionality', 'we', 'demonstrated', 'that', 'the', 'word', 'and', 'phrase', 'representations', 'learned', 'by', 'the', 'skip-gram', 'model', 'exhibit', 'a', 'linear', 'structure', 'that', 'makes', 'it', 'possible', 'to', 'perform', 'precise', 'analogical', 'reasoning', 'using', 'simple', 'vector', 'arithmetics', 'interestingly', 'we', 'found', 'that', 'the', 'skip-gram', 'representations', 'exhibit', 'another', 'kind', 'of', 'linear', 'structure', 'that', 'makes', 'it', 'possible', 'to', 'meaningfully', 'combine', 'words', 'by', 'an', 'element-wise', 'addition', 'of', 'their', 'vector', 'representations', 'this', 'phenomenon', 'is', 'illustrated', 'in', 'table', 'the', 'additive', 'property', 'of', 'the', 'vectors', 'can', 'be', 'explained', 'by', 'inspecting', 'the', 'training', 'objective', 'the', 'word', 'vectors', 'are', 'in', 'a', 'linear', 'relationship', 'with', 'the', 'inputs', 'to', 'the', 'softmax', 'nonlinearity', 'as', 'the', 'word', 'vectors', 'are', 'trained', 'to', 'predict', 'the', 'surrounding', 'words', 'in', 'the', 'sentence', 'the', 'vectors', 'can', 'be', 'seen', 'as', 'representing', 'the', 'distribution', 'of', 'the', 'context', 'in', 'which', 'a', 'word', 'appears', 'these', 'values', 'are', 'related', 'logarithmically', 'to', 'the', 'probabilities', 'computed', 'by', 'the', 'output', 'layer', 'so', 'the', 'sum', 'of', 'two', 'word', 'vectors', 'is', 'related', 'to', 'the', 'product', 'of', 'the', 'two', 'context', 'distributions', 'the', 'product', 'works', 'here', 'as', 'the', 'and', 'function', 'words', 'that', 'are', 'assigned', 'high', 'probabilities', 'by', 'both', 'word', 'vectors', 'will', 'have', 'high', 'probability', 'and', 'the', 'other', 'words', 'will', 'have', 'low', 'probability', 'thus', 'if', 'volga', 'river', 'appears', 'frequently', 'in', 'the', 'same', 'sentence', 'together', 'with', 'the', 'words', 'russian', 'and', 'river', 'the', 'sum', 'of', 'these', 'two', 'word', 'vectors', 'will', 'result', 'in', 'such', 'a', 'feature', 'vector', 'that', 'is', 'close', 'to', 'the', 'vector', 'of', 'volga', 'river', 'comparison', 'to', 'published', 'word', 'representations', 'many', 'authors', 'who', 'previously', 'worked', 'on', 'the', 'neural', 'network', 'based', 'representations', 'of', 'words', 'have', 'published', 'their', 'resulting', 'models', 'for', 'further', 'use', 'and', 'comparison', 'amongst', 'the', 'most', 'well', 'known', 'authors', 'are', 'collobert', 'and', 'weston', 'turian', 'et', 'al', 'and', 'mnih', 'and', 'hinton', 'we', 'downloaded', 'their', 'word', 'vectors', 'from', 'the', 'web3', 'mikolov', 'et', 'al', 'have', 'already', 'evaluated', 'these', 'word', 'representations', 'on', 'the', 'word', 'analogy', 'task', 'where', 'the', 'skip-gram', 'models', 'achieved', 'the', 'best', 'performance', 'with', 'a', 'huge', 'margin', '3http', 'metaoptimize', 'com', 'projects', 'wordreprs', 'model', 'redmond', 'havel', 'ninjutsu', 'graffiti', 'capitulate', 'training', 'time', 'collobert', '50d', 'conyers', 'plauen', 'reiki', 'cheesecake', 'abdicate', 'months', 'lubbock', 'dzerzhinsky', 'kohona', 'gossip', 'accede', 'keene', 'osterreich', 'karate', 'dioramas', 'rearm', 'turian', '200d', 'mccarthy', 'jewell', '-gunfire', '-', 'few', 'weeks', 'alston', 'arzu', '-emotion', '-', 'cousins', 'ovitz', '-impunity', '-', 'mnih', '100d', 'podhurst', 'pontiff', '-anaesthetics', 'mavericks', 'days', 'harlang', 'pinochet', '-monkeys', 'planning', 'agarwal', 'rodionov', '-jews', 'hesitated', 'skip-phrase', 'redmond', 'wash', 'vaclav', 'havel', 'ninja', 'spray', 'paint', 'capitulation', '1000d', 'day', 'redmond', 'washington', 'president', 'vaclav', 'havel', 'martial', 'arts', 'grafitti', 'capitulated', 'microsoft', 'velvet', 'revolution', 'swordsmanship', 'taggers', 'capitulating', 'table', 'examples', 'of', 'the', 'closest', 'tokens', 'given', 'various', 'well', 'known', 'models', 'and', 'the', 'skip-gram', 'model', 'trained', 'on', 'phrases', 'using', 'over', 'billion', 'training', 'words', 'an', 'empty', 'cell', 'means', 'that', 'the', 'word', 'was', 'not', 'in', 'the', 'vocabulary', 'to', 'give', 'more', 'insight', 'into', 'the', 'difference', 'of', 'the', 'quality', 'of', 'the', 'learned', 'vectors', 'we', 'provide', 'empirical', 'comparison', 'by', 'showing', 'the', 'nearest', 'neighbours', 'of', 'infrequent', 'words', 'in', 'table', 'these', 'examples', 'show', 'that', 'the', 'big', 'skip-gram', 'model', 'trained', 'on', 'a', 'large', 'corpus', 'visibly', 'outperforms', 'all', 'the', 'other', 'models', 'in', 'the', 'quality', 'of', 'the', 'learned', 'representations', 'this', 'can', 'be', 'attributed', 'in', 'part', 'to', 'the', 'fact', 'that', 'this', 'model', 'has', 'been', 'trained', 'on', 'about', 'billion', 'words', 'which', 'is', 'about', 'two', 'to', 'three', 'orders', 'of', 'magnitude', 'more', 'data', 'than', 'the', 'typical', 'size', 'used', 'in', 'the', 'prior', 'work', 'interestingly', 'although', 'the', 'training', 'set', 'is', 'much', 'larger', 'the', 'training', 'time', 'of', 'the', 'skip-gram', 'model', 'is', 'just', 'a', 'fraction', 'of', 'the', 'time', 'complexity', 'required', 'by', 'the', 'previous', 'model', 'architectures', 'conclusion', 'this', 'work', 'has', 'several', 'key', 'contributions', 'we', 'show', 'how', 'to', 'train', 'distributed', 'representations', 'of', 'words', 'and', 'phrases', 'with', 'the', 'skip-gram', 'model', 'and', 'demonstrate', 'that', 'these', 'representations', 'exhibit', 'linear', 'structure', 'that', 'makes', 'precise', 'analogical', 'reasoning', 'possible', 'the', 'techniques', 'introduced', 'in', 'this', 'paper', 'can', 'be', 'used', 'also', 'for', 'training', 'the', 'continuous', 'bag-of-words', 'model', 'introduced', 'in', 'we', 'successfully', 'trained', 'models', 'on', 'several', 'orders', 'of', 'magnitude', 'more', 'data', 'than', 'the', 'previously', 'published', 'models', 'thanks', 'to', 'the', 'computationally', 'efficient', 'model', 'architecture', 'this', 'results', 'in', 'a', 'great', 'improvement', 'in', 'the', 'quality', 'of', 'the', 'learned', 'word', 'and', 'phrase', 'representations', 'especially', 'for', 'the', 'rare', 'entities', 'we', 'also', 'found', 'that', 'the', 'subsampling', 'of', 'the', 'frequent', 'words', 'results', 'in', 'both', 'faster', 'training', 'and', 'significantly', 'better', 'representations', 'of', 'uncommon', 'words', 'another', 'contribution', 'of', 'our', 'paper', 'is', 'the', 'negative', 'sampling', 'algorithm', 'which', 'is', 'an', 'extremely', 'simple', 'training', 'method', 'that', 'learns', 'accurate', 'representations', 'especially', 'for', 'frequent', 'words', 'the', 'choice', 'of', 'the', 'training', 'algorithm', 'and', 'the', 'hyper-parameter', 'selection', 'is', 'a', 'task', 'specific', 'decision', 'as', 'we', 'found', 'that', 'different', 'problems', 'have', 'different', 'optimal', 'hyperparameter', 'configurations', 'in', 'our', 'experiments', 'the', 'most', 'crucial', 'decisions', 'that', 'affect', 'the', 'performance', 'are', 'the', 'choice', 'of', 'the', 'model', 'architecture', 'the', 'size', 'of', 'the', 'vectors', 'the', 'subsampling', 'rate', 'and', 'the', 'size', 'of', 'the', 'training', 'window', 'a', 'very', 'interesting', 'result', 'of', 'this', 'work', 'is', 'that', 'the', 'word', 'vectors', 'can', 'be', 'somewhat', 'meaningfully', 'combined', 'using', 'just', 'simple', 'vector', 'addition', 'another', 'approach', 'for', 'learning', 'representations', 'of', 'phrases', 'presented', 'in', 'this', 'paper', 'is', 'to', 'simply', 'represent', 'the', 'phrases', 'with', 'a', 'single', 'token', 'combination', 'of', 'these', 'two', 'approaches', 'gives', 'a', 'powerful', 'yet', 'simple', 'way', 'how', 'to', 'represent', 'longer', 'pieces', 'of', 'text', 'while', 'having', 'minimal', 'computational', 'complexity', 'our', 'work', 'can', 'thus', 'be', 'seen', 'as', 'complementary', 'to', 'the', 'existing', 'approach', 'that', 'attempts', 'to', 'represent', 'phrases', 'using', 'recursive', 'matrix-vector', 'operations', 'we', 'made', 'the', 'code', 'for', 'training', 'the', 'word', 'and', 'phrase', 'vectors', 'based', 'on', 'the', 'techniques', 'described', 'in', 'this', 'paper', 'available', 'as', 'an', 'open-source', 'project4', '4code', 'google', 'com', 'p', 'word2vec', 'references', 'yoshua', 'bengio', 'r', 'ejean', 'ducharme', 'pascal', 'vincent', 'and', 'christian', 'janvin', 'a', 'neural', 'probabilistic', 'language', 'model', 'the', 'journal', 'of', 'machine', 'learning', 'research', 'ronan', 'collobert', 'and', 'jason', 'weston', 'a', 'unified', 'architecture', 'for', 'natural', 'language', 'processing', 'deep', 'neural', 'networks', 'with', 'multitask', 'learning', 'in', 'proceedings', 'of', 'the', '25th', 'international', 'conference', 'on', 'machine', 'learning', 'pages', 'acm', 'xavier', 'glorot', 'antoine', 'bordes', 'and', 'yoshua', 'bengio', 'domain', 'adaptation', 'for', 'large-scale', 'sentiment', 'classification', 'a', 'deep', 'learning', 'approach', 'in', 'icml', 'michael', 'u', 'gutmann', 'and', 'aapo', 'hyv', 'arinen', 'noise-contrastive', 'estimation', 'of', 'unnormalized', 'statistical', 'models', 'with', 'applications', 'to', 'natural', 'image', 'statistics', 'the', 'journal', 'of', 'machine', 'learning', 'research', 'tomas', 'mikolov', 'stefan', 'kombrink', 'lukas', 'burget', 'jan', 'cernocky', 'and', 'sanjeev', 'khudanpur', 'extensions', 'of', 'recurrent', 'neural', 'network', 'language', 'model', 'in', 'acoustics', 'speech', 'and', 'signal', 'processing', 'icassp', 'ieee', 'international', 'conference', 'on', 'pages', 'ieee', 'tomas', 'mikolov', 'anoop', 'deoras', 'daniel', 'povey', 'lukas', 'burget', 'and', 'jan', 'cernocky', 'strategies', 'for', 'training', 'large', 'scale', 'neural', 'network', 'language', 'models', 'in', 'proc', 'automatic', 'speech', 'recognition', 'and', 'understanding', 'tomas', 'mikolov', 'statistical', 'language', 'models', 'based', 'on', 'neural', 'networks', 'phd', 'thesis', 'phd', 'thesis', 'brno', 'university', 'of', 'technology', 'tomas', 'mikolov', 'kai', 'chen', 'greg', 'corrado', 'and', 'jeffrey', 'dean', 'efficient', 'estimation', 'of', 'word', 'representations', 'in', 'vector', 'space', 'iclr', 'workshop', 'tomas', 'mikolov', 'wen-tau', 'yih', 'and', 'geoffrey', 'zweig', 'linguistic', 'regularities', 'in', 'continuous', 'space', 'word', 'representations', 'in', 'proceedings', 'of', 'naacl', 'hlt', 'andriy', 'mnih', 'and', 'geoffrey', 'e', 'hinton', 'a', 'scalable', 'hierarchical', 'distributed', 'language', 'model', 'advances', 'in', 'neural', 'information', 'processing', 'systems', 'andriy', 'mnih', 'and', 'yee', 'whye', 'teh', 'a', 'fast', 'and', 'simple', 'algorithm', 'for', 'training', 'neural', 'probabilistic', 'language', 'models', 'arxiv', 'preprint', 'arxiv', 'frederic', 'morin', 'and', 'yoshua', 'bengio', 'hierarchical', 'probabilistic', 'neural', 'network', 'language', 'model', 'in', 'proceedings', 'of', 'the', 'international', 'workshop', 'on', 'artificial', 'intelligence', 'and', 'statistics', 'pages', 'david', 'e', 'rumelhart', 'geoffrey', 'e', 'hintont', 'and', 'ronald', 'j', 'williams', 'learning', 'representations', 'by', 'backpropagating', 'errors', 'nature', 'holger', 'schwenk', 'continuous', 'space', 'language', 'models', 'computer', 'speech', 'and', 'language', 'vol', 'richard', 'socher', 'cliff', 'c', 'lin', 'andrew', 'y', 'ng', 'and', 'christopher', 'd', 'manning', 'parsing', 'natural', 'scenes', 'and', 'natural', 'language', 'with', 'recursive', 'neural', 'networks', 'in', 'proceedings', 'of', 'the', '26th', 'international', 'conference', 'on', 'machine', 'learning', 'icml', 'volume', 'richard', 'socher', 'brody', 'huval', 'christopher', 'd', 'manning', 'and', 'andrew', 'y', 'ng', 'semantic', 'compositionality', 'through', 'recursive', 'matrix-vector', 'spaces', 'in', 'proceedings', 'of', 'the', 'conference', 'on', 'empirical', 'methods', 'in', 'natural', 'language', 'processing', 'emnlp', 'joseph', 'turian', 'lev', 'ratinov', 'and', 'yoshua', 'bengio', 'word', 'representations', 'a', 'simple', 'and', 'general', 'method', 'for', 'semi-supervised', 'learning', 'in', 'proceedings', 'of', 'the', '48th', 'annual', 'meeting', 'of', 'the', 'association', 'for', 'computational', 'linguistics', 'pages', 'association', 'for', 'computational', 'linguistics', 'peter', 'd', 'turney', 'and', 'patrick', 'pantel', 'from', 'frequency', 'to', 'meaning', 'vector', 'space', 'models', 'of', 'semantics', 'in', 'journal', 'of', 'artificial', 'intelligence', 'research', '141-188', 'peter', 'd', 'turney', 'distributional', 'semantics', 'beyond', 'words', 'supervised', 'learning', 'of', 'analogy', 'and', 'paraphrase', 'in', 'transactions', 'of', 'the', 'association', 'for', 'computational', 'linguistics', 'tacl', 'jason', 'weston', 'samy', 'bengio', 'and', 'nicolas', 'usunier', 'wsabie', 'scaling', 'up', 'to', 'large', 'vocabulary', 'image', 'annotation', 'in', 'proceedings', 'of', 'the', 'twenty-second', 'international', 'joint', 'conference', 'on', 'artificial', 'intelligence-volume', 'volume', 'three', 'pages', 'aaai', 'press']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop-words. \\\\\n",
        "Initially I decided to take stop-word list simply from NLTK and put as list here, but found extended version on https://gist.github.com/rg089/35e00abf8941d72d419224cfd5b5925d \\\\\n",
        "I added article-specific words like research (and synonyms) and standard names of sections"
      ],
      "metadata": {
        "id": "H6tQl0uq735z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\", \"study\", \"paper\", \"article\", \"research\", \"abstract\", \"introduction\", \"methods\", \"conclusion\", \"results\", \"references\"]"
      ],
      "metadata": {
        "id": "4rExvU38nQlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [word for word in text if word not in stop_words]\n",
        "\n",
        "print(text)\n",
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDHulqn8ni1s",
        "outputId": "b847185e-0370-441d-d921-4c43991ac4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['arxiv', '4546v1', 'oct', 'distributed', 'representations', 'phrases', 'compositionality', 'tomas', 'mikolov', 'google', 'mountain', 'view', 'mikolov', 'google', 'ilya', 'sutskever', 'google', 'mountain', 'view', 'ilyasu', 'google', 'kai', 'chen', 'google', 'mountain', 'view', 'kai', 'google', 'greg', 'corrado', 'google', 'mountain', 'view', 'gcorrado', 'google', 'jeffrey', 'dean', 'google', 'mountain', 'view', 'jeff', 'google', 'introduced', 'continuous', 'skip-gram', 'model', 'efficient', 'method', 'learning', 'high-quality', 'distributed', 'vector', 'representations', 'capture', 'large', 'number', 'precise', 'syntactic', 'semantic', 'word', 'relationships', 'extensions', 'improve', 'quality', 'vectors', 'training', 'speed', 'subsampling', 'frequent', 'speedup', 'learn', 'regular', 'word', 'representations', 'simple', 'alternative', 'hierarchical', 'softmax', 'called', 'negative', 'sampling', 'inherent', 'limitation', 'word', 'representations', 'indifference', 'word', 'order', 'inability', 'represent', 'idiomatic', 'phrases', 'meanings', 'canada', 'air', 'easily', 'combined', 'air', 'canada', 'motivated', 'simple', 'method', 'finding', 'phrases', 'text', 'learning', 'good', 'vector', 'representations', 'millions', 'phrases', 'distributed', 'representations', 'vector', 'space', 'learning', 'algorithms', 'achieve', 'performance', 'natural', 'language', 'processing', 'tasks', 'grouping', 'earliest', 'word', 'representations', 'dates', 'rumelhart', 'hinton', 'williams', 'idea', 'applied', 'statistical', 'language', 'modeling', 'considerable', 'success', 'follow', 'work', 'includes', 'applications', 'automatic', 'speech', 'recognition', 'machine', 'translation', 'wide', 'range', 'nlp', 'tasks', 'mikolov', 'introduced', 'skip-gram', 'model', 'efficient', 'method', 'learning', 'highquality', 'vector', 'representations', 'large', 'amounts', 'unstructured', 'text', 'data', 'neural', 'network', 'architectures', 'learning', 'word', 'vectors', 'training', 'skipgram', 'model', 'figure', 'involve', 'dense', 'matrix', 'multiplications', 'training', 'extremely', 'efficient', 'optimized', 'single-machine', 'implementation', 'train', 'billion', 'day', 'word', 'representations', 'computed', 'neural', 'networks', 'interesting', 'learned', 'vectors', 'explicitly', 'encode', 'linguistic', 'regularities', 'patterns', 'surprisingly', 'patterns', 'represented', 'linear', 'translations', 'result', 'vector', 'calculation', 'vec', 'madrid', '-', 'vec', 'spain', 'vec', 'france', 'closer', 'vec', 'paris', 'word', 'vector', 'figure', 'skip-gram', 'model', 'architecture', 'training', 'objective', 'learn', 'word', 'vector', 'representations', 'good', 'predicting', 'nearby', 'extensions', 'original', 'skip-gram', 'model', 'subsampling', 'frequent', 'training', 'speedup', '2x', '-', '10x', 'improves', 'accuracy', 'representations', 'frequent', 'addition', 'simplified', 'variant', 'noise', 'contrastive', 'estimation', 'nce', 'training', 'skip-gram', 'model', 'faster', 'training', 'vector', 'representations', 'frequent', 'compared', 'complex', 'hierarchical', 'softmax', 'prior', 'work', 'word', 'representations', 'limited', 'inability', 'represent', 'idiomatic', 'phrases', 'compositions', 'individual', 'boston', 'globe', 'newspaper', 'natural', 'combination', 'meanings', 'boston', 'globe', 'vectors', 'represent', 'phrases', 'skip-gram', 'model', 'considerably', 'expressive', 'techniques', 'aim', 'represent', 'meaning', 'sentences', 'composing', 'word', 'vectors', 'recursive', 'autoencoders', 'benefit', 'phrase', 'vectors', 'word', 'vectors', 'extension', 'word', 'based', 'phrase', 'based', 'models', 'simple', 'identify', 'large', 'number', 'phrases', 'data-driven', 'approach', 'treat', 'phrases', 'individual', 'tokens', 'training', 'evaluate', 'quality', 'phrase', 'vectors', 'developed', 'test', 'set', 'analogical', 'reasoning', 'tasks', 'phrases', 'typical', 'analogy', 'pair', 'test', 'set', 'montreal', 'montreal', 'canadiens', 'toronto', 'toronto', 'maple', 'leafs', 'considered', 'answered', 'correctly', 'nearest', 'representation', 'vec', 'montreal', 'canadiens', '-', 'vec', 'montreal', 'vec', 'toronto', 'vec', 'toronto', 'maple', 'leafs', 'finally', 'interesting', 'property', 'skip-gram', 'model', 'simple', 'vector', 'addition', 'produce', 'meaningful', 'vec', 'russia', 'vec', 'river', 'close', 'vec', 'volga', 'river', 'vec', 'germany', 'vec', 'capital', 'close', 'vec', 'berlin', 'compositionality', 'suggests', 'non-obvious', 'degree', 'language', 'understanding', 'basic', 'mathematical', 'operations', 'word', 'vector', 'representations', 'skip-gram', 'model', 'training', 'objective', 'skip-gram', 'model', 'word', 'representations', 'predicting', 'surrounding', 'sentence', 'document', 'formally', 'sequence', 'training', 'w1', 'w2', 'w3', 'wt', 'objective', 'skip-gram', 'model', 'maximize', 'average', 'log', 'probability', 'log', 'wt', 'wt', 'size', 'training', 'context', 'function', 'center', 'word', 'wt', 'larger', 'training', 'examples', 'lead', 'higher', 'accuracy', 'expense', 'training', 'time', 'basic', 'skip-gram', 'formulation', 'defines', 'wt', 'wt', 'softmax', 'function', 'exp', 'vwi', 'pw', 'exp', 'vwi', 'vw', 'input', 'output', 'vector', 'representations', 'number', 'vocabulary', 'formulation', 'impractical', 'cost', 'computing', 'log', 'proportional', 'large', 'terms', 'hierarchical', 'softmax', 'computationally', 'efficient', 'approximation', 'softmax', 'hierarchical', 'softmax', 'context', 'neural', 'network', 'language', 'models', 'introduced', 'morin', 'bengio', 'main', 'advantage', 'evaluating', 'output', 'nodes', 'neural', 'network', 'probability', 'distribution', 'needed', 'evaluate', 'log2', 'nodes', 'hierarchical', 'softmax', 'binary', 'tree', 'representation', 'output', 'layer', 'leaves', 'node', 'explicitly', 'represents', 'relative', 'probabilities', 'child', 'nodes', 'define', 'random', 'walk', 'assigns', 'probabilities', 'precisely', 'word', 'reached', 'path', 'root', 'tree', 'j-th', 'node', 'path', 'root', 'length', 'path', 'root', 'addition', 'node', 'arbitrary', 'fixed', 'child', 'true', '-1', 'hierarchical', 'softmax', 'defines', 'σ', 'vwi', 'σ', 'exp', 'verified', 'pw', 'implies', 'cost', 'computing', 'log', 'log', 'proportional', 'average', 'greater', 'log', 'standard', 'softmax', 'formulation', 'skip-gram', 'assigns', 'representations', 'vw', 'word', 'hierarchical', 'softmax', 'formulation', 'representation', 'vw', 'word', 'representation', 'node', 'binary', 'tree', 'structure', 'tree', 'hierarchical', 'softmax', 'considerable', 'performance', 'mnih', 'hinton', 'explored', 'number', 'constructing', 'tree', 'structure', 'training', 'time', 'model', 'accuracy', 'work', 'binary', 'huffman', 'tree', 'assigns', 'short', 'codes', 'frequent', 'fast', 'training', 'observed', 'grouping', 'frequency', 'works', 'simple', 'speedup', 'technique', 'neural', 'network', 'based', 'language', 'models', 'negative', 'sampling', 'alternative', 'hierarchical', 'softmax', 'noise', 'contrastive', 'estimation', 'nce', 'introduced', 'gutmann', 'hyvarinen', 'applied', 'language', 'modeling', 'mnih', 'teh', 'nce', 'posits', 'good', 'model', 'differentiate', 'data', 'noise', 'logistic', 'regression', 'hinge', 'loss', 'collobert', 'weston', 'trained', 'models', 'ranking', 'data', 'noise', 'nce', 'maximize', 'log', 'probability', 'softmax', 'skipgram', 'model', 'concerned', 'learning', 'high-quality', 'vector', 'representations', 'free', 'simplify', 'nce', 'long', 'vector', 'representations', 'retain', 'quality', 'define', 'negative', 'sampling', 'neg', 'objective', 'log', 'σ', 'vwi', 'ewi', 'log', 'σ', 'vwi', '-2', '-1', '-1', '-0', '-2', '-1', '-1', '-0', 'country', 'capital', 'vectors', 'projected', 'pca', 'china', 'japan', 'france', 'russia', 'germany', 'italy', 'spain', 'greece', 'turkey', 'beijing', 'paris', 'tokyo', 'poland', 'moscow', 'portugal', 'berlin', 'rome', 'athens', 'madrid', 'ankara', 'warsaw', 'lisbon', 'figure', 'two-dimensional', 'pca', 'projection', '1000-dimensional', 'skip-gram', 'vectors', 'countries', 'capital', 'cities', 'figure', 'illustrates', 'ability', 'model', 'automatically', 'organize', 'concepts', 'learn', 'implicitly', 'relationships', 'training', 'provide', 'supervised', 'capital', 'city', 'replace', 'log', 'term', 'skip-gram', 'objective', 'task', 'distinguish', 'target', 'word', 'draws', 'noise', 'distribution', 'logistic', 'regression', 'negative', 'samples', 'data', 'sample', 'experiments', 'values', 'range', 'small', 'training', 'datasets', 'large', 'datasets', 'small', 'main', 'difference', 'negative', 'sampling', 'nce', 'nce', 'samples', 'numerical', 'probabilities', 'noise', 'distribution', 'negative', 'sampling', 'samples', 'nce', 'maximizes', 'log', 'probability', 'softmax', 'property', 'application', 'nce', 'neg', 'noise', 'distribution', 'free', 'parameter', 'investigated', 'number', 'choices', 'unigram', 'distribution', 'raised', '4rd', 'power', 'outperformed', 'unigram', 'uniform', 'distributions', 'nce', 'neg', 'task', 'including', 'language', 'modeling', 'reported', 'subsampling', 'frequent', 'large', 'corpora', 'frequent', 'easily', 'occur', 'hundreds', 'millions', 'times', 'provide', 'rare', 'skip-gram', 'model', 'benefits', 'observing', 'co-occurrences', 'france', 'paris', 'benefits', 'observing', 'frequent', 'co-occurrences', 'france', 'word', 'co-occurs', 'frequently', 'sentence', 'idea', 'applied', 'opposite', 'direction', 'vector', 'representations', 'frequent', 'change', 'training', 'examples', 'counter', 'imbalance', 'rare', 'frequent', 'simple', 'subsampling', 'approach', 'word', 'training', 'set', 'discarded', 'probability', 'computed', 'formula', 'method', 'time', 'min', 'syntactic', 'semantic', 'total', 'accuracy', 'neg-5', 'neg-15', 'hs-huffman', 'nce-5', 'subsampling', 'neg-5', 'neg-15', 'hs-huffman', 'table', 'accuracy', 'skip-gram', '300-dimensional', 'models', 'analogical', 'reasoning', 'task', 'defined', 'neg-k', 'stands', 'negative', 'sampling', 'negative', 'samples', 'positive', 'sample', 'nce', 'stands', 'noise', 'contrastive', 'estimation', 'hs-huffman', 'stands', 'hierarchical', 'softmax', 'frequency-based', 'huffman', 'codes', 'frequency', 'word', 'chosen', 'threshold', 'typically', 'chose', 'subsampling', 'formula', 'aggressively', 'subsamples', 'frequency', 'greater', 'preserving', 'ranking', 'frequencies', 'subsampling', 'formula', 'chosen', 'heuristically', 'work', 'practice', 'accelerates', 'learning', 'improves', 'accuracy', 'learned', 'vectors', 'rare', 'sections', 'empirical', 'evaluate', 'hierarchical', 'softmax', 'noise', 'contrastive', 'estimation', 'negative', 'sampling', 'subsampling', 'training', 'analogical', 'reasoning', 'task1', 'introduced', 'mikolov', 'task', 'consists', 'analogies', 'germany', 'berlin', 'france', 'solved', 'finding', 'vector', 'vec', 'closest', 'vec', 'berlin', '-', 'vec', 'germany', 'vec', 'france', 'cosine', 'distance', 'discard', 'input', 'search', 'specific', 'considered', 'answered', 'correctly', 'paris', 'task', 'broad', 'categories', 'syntactic', 'analogies', 'quick', 'slow', 'slowly', 'semantic', 'analogies', 'country', 'capital', 'city', 'relationship', 'training', 'skip-gram', 'models', 'large', 'dataset', 'consisting', 'news', 'articles', 'internal', 'google', 'dataset', 'billion', 'discarded', 'vocabulary', 'occurred', 'times', 'training', 'data', 'vocabulary', 'size', '692k', 'performance', 'skip-gram', 'models', 'word', 'analogy', 'test', 'set', 'reported', 'table', 'table', 'negative', 'sampling', 'outperforms', 'hierarchical', 'softmax', 'analogical', 'reasoning', 'task', 'performance', 'noise', 'contrastive', 'estimation', 'subsampling', 'frequent', 'improves', 'training', 'speed', 'times', 'word', 'representations', 'accurate', 'argued', 'linearity', 'skip-gram', 'model', 'vectors', 'suitable', 'linear', 'analogical', 'reasoning', 'mikolov', 'vectors', 'learned', 'standard', 'sigmoidal', 'recurrent', 'neural', 'networks', 'highly', 'non-linear', 'improve', 'task', 'training', 'data', 'increases', 'suggesting', 'non-linear', 'models', 'preference', 'linear', 'structure', 'word', 'representations', 'learning', 'phrases', 'discussed', 'earlier', 'phrases', 'meaning', 'simple', 'composition', 'meanings', 'individual', 'learn', 'vector', 'representation', 'phrases', 'frequently', 'infrequently', 'contexts', 'york', 'times', 'toronto', 'maple', 'leafs', 'replaced', 'unique', 'tokens', 'training', 'data', 'bigram', 'remain', 'unchanged', '1code', 'google', 'word2vec', 'source', 'browse', 'trunk', 'questions-words', 'txt', 'newspapers', 'york', 'york', 'times', 'baltimore', 'baltimore', 'sun', 'san', 'jose', 'san', 'jose', 'mercury', 'news', 'cincinnati', 'cincinnati', 'enquirer', 'nhl', 'teams', 'boston', 'boston', 'bruins', 'montreal', 'montreal', 'canadiens', 'phoenix', 'phoenix', 'coyotes', 'nashville', 'nashville', 'predators', 'nba', 'teams', 'detroit', 'detroit', 'pistons', 'toronto', 'toronto', 'raptors', 'oakland', 'golden', 'state', 'warriors', 'memphis', 'memphis', 'grizzlies', 'airlines', 'austria', 'austrian', 'airlines', 'spain', 'spainair', 'belgium', 'brussels', 'airlines', 'greece', 'aegean', 'airlines', 'company', 'executives', 'steve', 'ballmer', 'microsoft', 'larry', 'google', 'samuel', 'palmisano', 'ibm', 'werner', 'vogels', 'amazon', 'table', 'examples', 'analogical', 'reasoning', 'task', 'phrases', 'test', 'set', 'examples', 'goal', 'compute', 'fourth', 'phrase', 'model', 'achieved', 'accuracy', 'dataset', 'form', 'reasonable', 'phrases', 'greatly', 'increasing', 'size', 'vocabulary', 'theory', 'train', 'skip-gram', 'model', 'n-grams', 'memory', 'intensive', 'techniques', 'developed', 'identify', 'phrases', 'text', 'scope', 'work', 'compare', 'decided', 'simple', 'data-driven', 'approach', 'phrases', 'formed', 'based', 'unigram', 'bigram', 'counts', 'score', 'wj', 'count', 'wiwj', 'δ', 'count', 'count', 'wj', 'δ', 'discounting', 'coefficient', 'prevents', 'phrases', 'consisting', 'infrequent', 'formed', 'bigrams', 'score', 'chosen', 'threshold', 'phrases', 'typically', '2-4', 'passes', 'training', 'data', 'decreasing', 'threshold', 'allowing', 'longer', 'phrases', 'consists', 'formed', 'evaluate', 'quality', 'phrase', 'representations', 'analogical', 'reasoning', 'task', 'involves', 'phrases', 'table', 'examples', 'categories', 'analogies', 'task', 'dataset', 'publicly', 'web2', 'phrase', 'skip-gram', 'starting', 'news', 'data', 'previous', 'experiments', 'constructed', 'phrase', 'based', 'training', 'corpus', 'trained', 'skip-gram', 'models', 'hyperparameters', 'vector', 'dimensionality', 'context', 'size', 'setting', 'achieves', 'good', 'performance', 'phrase', 'dataset', 'allowed', 'compare', 'negative', 'sampling', 'hierarchical', 'softmax', 'subsampling', 'frequent', 'tokens', 'summarized', 'table', 'negative', 'sampling', 'achieves', 'respectable', 'accuracy', 'achieves', 'considerably', 'performance', 'surprisingly', 'hierarchical', 'softmax', 'achieve', 'lower', 'performance', 'trained', 'subsampling', 'performing', 'method', 'downsampled', 'frequent', 'subsampling', 'result', 'faster', 'training', 'improve', 'accuracy', 'cases', '2code', 'google', 'word2vec', 'source', 'browse', 'trunk', 'questions-phrases', 'txt', 'method', 'dimensionality', 'subsampling', 'subsampling', 'neg-5', 'neg-15', 'hs-huffman', 'table', 'accuracies', 'skip-gram', 'models', 'phrase', 'analogy', 'dataset', 'models', 'trained', 'billion', 'news', 'dataset', 'neg-15', 'subsampling', 'subsampling', 'vasco', 'gama', 'lingsugur', 'italian', 'explorer', 'lake', 'baikal', 'great', 'rift', 'valley', 'aral', 'sea', 'alan', 'bean', 'rebbeca', 'naomi', 'moonwalker', 'ionian', 'sea', 'ruegen', 'ionian', 'islands', 'chess', 'master', 'chess', 'grandmaster', 'garry', 'kasparov', 'table', 'examples', 'closest', 'entities', 'short', 'phrases', 'models', 'czech', 'currency', 'vietnam', 'capital', 'german', 'airlines', 'russian', 'river', 'french', 'actress', 'koruna', 'hanoi', 'airline', 'lufthansa', 'moscow', 'juliette', 'binoche', 'check', 'crown', 'chi', 'minh', 'city', 'carrier', 'lufthansa', 'volga', 'river', 'vanessa', 'paradis', 'polish', 'zolty', 'viet', 'nam', 'flag', 'carrier', 'lufthansa', 'upriver', 'charlotte', 'gainsbourg', 'ctk', 'vietnamese', 'lufthansa', 'russia', 'cecile', 'table', 'vector', 'compositionality', 'element-wise', 'addition', 'closest', 'tokens', 'sum', 'vectors', 'skip-gram', 'model', 'maximize', 'accuracy', 'phrase', 'analogy', 'task', 'increased', 'training', 'data', 'dataset', 'billion', 'hierarchical', 'softmax', 'dimensionality', 'entire', 'sentence', 'context', 'model', 'reached', 'accuracy', 'achieved', 'lower', 'accuracy', 'reduced', 'size', 'training', 'dataset', 'suggests', 'large', 'training', 'data', 'crucial', 'gain', 'insight', 'representations', 'learned', 'models', 'inspect', 'manually', 'nearest', 'neighbours', 'infrequent', 'phrases', 'models', 'table', 'sample', 'comparison', 'consistently', 'previous', 'representations', 'phrases', 'learned', 'model', 'hierarchical', 'softmax', 'subsampling', 'additive', 'compositionality', 'demonstrated', 'word', 'phrase', 'representations', 'learned', 'skip-gram', 'model', 'exhibit', 'linear', 'structure', 'perform', 'precise', 'analogical', 'reasoning', 'simple', 'vector', 'arithmetics', 'interestingly', 'skip-gram', 'representations', 'exhibit', 'kind', 'linear', 'structure', 'meaningfully', 'combine', 'element-wise', 'addition', 'vector', 'representations', 'phenomenon', 'illustrated', 'table', 'additive', 'property', 'vectors', 'explained', 'inspecting', 'training', 'objective', 'word', 'vectors', 'linear', 'relationship', 'inputs', 'softmax', 'nonlinearity', 'word', 'vectors', 'trained', 'predict', 'surrounding', 'sentence', 'vectors', 'representing', 'distribution', 'context', 'word', 'appears', 'values', 'logarithmically', 'probabilities', 'computed', 'output', 'layer', 'sum', 'word', 'vectors', 'product', 'context', 'distributions', 'product', 'works', 'function', 'assigned', 'high', 'probabilities', 'word', 'vectors', 'high', 'probability', 'low', 'probability', 'volga', 'river', 'appears', 'frequently', 'sentence', 'russian', 'river', 'sum', 'word', 'vectors', 'result', 'feature', 'vector', 'close', 'vector', 'volga', 'river', 'comparison', 'published', 'word', 'representations', 'authors', 'worked', 'neural', 'network', 'based', 'representations', 'published', 'models', 'comparison', 'authors', 'collobert', 'weston', 'turian', 'mnih', 'hinton', 'downloaded', 'word', 'vectors', 'web3', 'mikolov', 'evaluated', 'word', 'representations', 'word', 'analogy', 'task', 'skip-gram', 'models', 'achieved', 'performance', 'huge', 'margin', '3http', 'metaoptimize', 'projects', 'wordreprs', 'model', 'redmond', 'havel', 'ninjutsu', 'graffiti', 'capitulate', 'training', 'time', 'collobert', '50d', 'conyers', 'plauen', 'reiki', 'cheesecake', 'abdicate', 'months', 'lubbock', 'dzerzhinsky', 'kohona', 'gossip', 'accede', 'keene', 'osterreich', 'karate', 'dioramas', 'rearm', 'turian', '200d', 'mccarthy', 'jewell', '-gunfire', '-', 'weeks', 'alston', 'arzu', '-emotion', '-', 'cousins', 'ovitz', '-impunity', '-', 'mnih', '100d', 'podhurst', 'pontiff', '-anaesthetics', 'mavericks', 'days', 'harlang', 'pinochet', '-monkeys', 'planning', 'agarwal', 'rodionov', '-jews', 'hesitated', 'skip-phrase', 'redmond', 'wash', 'vaclav', 'havel', 'ninja', 'spray', 'paint', 'capitulation', '1000d', 'day', 'redmond', 'washington', 'president', 'vaclav', 'havel', 'martial', 'arts', 'grafitti', 'capitulated', 'microsoft', 'velvet', 'revolution', 'swordsmanship', 'taggers', 'capitulating', 'table', 'examples', 'closest', 'tokens', 'models', 'skip-gram', 'model', 'trained', 'phrases', 'billion', 'training', 'cell', 'word', 'vocabulary', 'insight', 'difference', 'quality', 'learned', 'vectors', 'provide', 'empirical', 'comparison', 'showing', 'nearest', 'neighbours', 'infrequent', 'table', 'examples', 'big', 'skip-gram', 'model', 'trained', 'large', 'corpus', 'visibly', 'outperforms', 'models', 'quality', 'learned', 'representations', 'attributed', 'fact', 'model', 'trained', 'billion', 'orders', 'magnitude', 'data', 'typical', 'size', 'prior', 'work', 'interestingly', 'training', 'set', 'larger', 'training', 'time', 'skip-gram', 'model', 'fraction', 'time', 'complexity', 'required', 'previous', 'model', 'architectures', 'work', 'key', 'contributions', 'train', 'distributed', 'representations', 'phrases', 'skip-gram', 'model', 'demonstrate', 'representations', 'exhibit', 'linear', 'structure', 'precise', 'analogical', 'reasoning', 'techniques', 'introduced', 'training', 'continuous', 'bag-of-words', 'model', 'introduced', 'trained', 'models', 'orders', 'magnitude', 'data', 'published', 'models', 'computationally', 'efficient', 'model', 'architecture', 'great', 'improvement', 'quality', 'learned', 'word', 'phrase', 'representations', 'rare', 'entities', 'subsampling', 'frequent', 'faster', 'training', 'representations', 'uncommon', 'contribution', 'negative', 'sampling', 'algorithm', 'extremely', 'simple', 'training', 'method', 'learns', 'accurate', 'representations', 'frequent', 'choice', 'training', 'algorithm', 'hyper-parameter', 'selection', 'task', 'specific', 'decision', 'problems', 'optimal', 'hyperparameter', 'configurations', 'experiments', 'crucial', 'decisions', 'affect', 'performance', 'choice', 'model', 'architecture', 'size', 'vectors', 'subsampling', 'rate', 'size', 'training', 'window', 'interesting', 'result', 'work', 'word', 'vectors', 'meaningfully', 'combined', 'simple', 'vector', 'addition', 'approach', 'learning', 'representations', 'phrases', 'presented', 'simply', 'represent', 'phrases', 'single', 'token', 'combination', 'approaches', 'powerful', 'simple', 'represent', 'longer', 'pieces', 'text', 'minimal', 'computational', 'complexity', 'work', 'complementary', 'existing', 'approach', 'attempts', 'represent', 'phrases', 'recursive', 'matrix-vector', 'operations', 'code', 'training', 'word', 'phrase', 'vectors', 'based', 'techniques', 'open-source', 'project4', '4code', 'google', 'word2vec', 'yoshua', 'bengio', 'ejean', 'ducharme', 'pascal', 'vincent', 'christian', 'janvin', 'neural', 'probabilistic', 'language', 'model', 'journal', 'machine', 'learning', 'ronan', 'collobert', 'jason', 'weston', 'unified', 'architecture', 'natural', 'language', 'processing', 'deep', 'neural', 'networks', 'multitask', 'learning', 'proceedings', '25th', 'international', 'conference', 'machine', 'learning', 'acm', 'xavier', 'glorot', 'antoine', 'bordes', 'yoshua', 'bengio', 'domain', 'adaptation', 'large-scale', 'sentiment', 'classification', 'deep', 'learning', 'approach', 'icml', 'michael', 'gutmann', 'aapo', 'hyv', 'arinen', 'noise-contrastive', 'estimation', 'unnormalized', 'statistical', 'models', 'applications', 'natural', 'image', 'statistics', 'journal', 'machine', 'learning', 'tomas', 'mikolov', 'stefan', 'kombrink', 'lukas', 'burget', 'jan', 'cernocky', 'sanjeev', 'khudanpur', 'extensions', 'recurrent', 'neural', 'network', 'language', 'model', 'acoustics', 'speech', 'signal', 'processing', 'icassp', 'ieee', 'international', 'conference', 'ieee', 'tomas', 'mikolov', 'anoop', 'deoras', 'daniel', 'povey', 'lukas', 'burget', 'jan', 'cernocky', 'strategies', 'training', 'large', 'scale', 'neural', 'network', 'language', 'models', 'proc', 'automatic', 'speech', 'recognition', 'understanding', 'tomas', 'mikolov', 'statistical', 'language', 'models', 'based', 'neural', 'networks', 'phd', 'thesis', 'phd', 'thesis', 'brno', 'university', 'technology', 'tomas', 'mikolov', 'kai', 'chen', 'greg', 'corrado', 'jeffrey', 'dean', 'efficient', 'estimation', 'word', 'representations', 'vector', 'space', 'iclr', 'workshop', 'tomas', 'mikolov', 'wen-tau', 'yih', 'geoffrey', 'zweig', 'linguistic', 'regularities', 'continuous', 'space', 'word', 'representations', 'proceedings', 'naacl', 'hlt', 'andriy', 'mnih', 'geoffrey', 'hinton', 'scalable', 'hierarchical', 'distributed', 'language', 'model', 'advances', 'neural', 'processing', 'systems', 'andriy', 'mnih', 'yee', 'whye', 'teh', 'fast', 'simple', 'algorithm', 'training', 'neural', 'probabilistic', 'language', 'models', 'arxiv', 'preprint', 'arxiv', 'frederic', 'morin', 'yoshua', 'bengio', 'hierarchical', 'probabilistic', 'neural', 'network', 'language', 'model', 'proceedings', 'international', 'workshop', 'artificial', 'intelligence', 'statistics', 'david', 'rumelhart', 'geoffrey', 'hintont', 'ronald', 'williams', 'learning', 'representations', 'backpropagating', 'errors', 'nature', 'holger', 'schwenk', 'continuous', 'space', 'language', 'models', 'computer', 'speech', 'language', 'richard', 'socher', 'cliff', 'lin', 'andrew', 'christopher', 'manning', 'parsing', 'natural', 'scenes', 'natural', 'language', 'recursive', 'neural', 'networks', 'proceedings', '26th', 'international', 'conference', 'machine', 'learning', 'icml', 'volume', 'richard', 'socher', 'brody', 'huval', 'christopher', 'manning', 'andrew', 'semantic', 'compositionality', 'recursive', 'matrix-vector', 'spaces', 'proceedings', 'conference', 'empirical', 'natural', 'language', 'processing', 'emnlp', 'joseph', 'turian', 'lev', 'ratinov', 'yoshua', 'bengio', 'word', 'representations', 'simple', 'general', 'method', 'semi-supervised', 'learning', 'proceedings', '48th', 'annual', 'meeting', 'association', 'computational', 'linguistics', 'association', 'computational', 'linguistics', 'peter', 'turney', 'patrick', 'pantel', 'frequency', 'meaning', 'vector', 'space', 'models', 'semantics', 'journal', 'artificial', 'intelligence', '141-188', 'peter', 'turney', 'distributional', 'semantics', 'supervised', 'learning', 'analogy', 'paraphrase', 'transactions', 'association', 'computational', 'linguistics', 'tacl', 'jason', 'weston', 'samy', 'bengio', 'nicolas', 'usunier', 'wsabie', 'scaling', 'large', 'vocabulary', 'image', 'annotation', 'proceedings', 'twenty-second', 'international', 'joint', 'conference', 'artificial', 'intelligence-volume', 'volume', 'aaai', 'press']\n",
            "2308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Bag of Words using NumPy"
      ],
      "metadata": {
        "id": "KOnQVkudmjk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy tokenized text from 1 step and create matrix"
      ],
      "metadata": {
        "id": "3oYUgSddsynq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "tokenized = text.copy()\n",
        "vocab = sorted(set(tokenized))\n",
        "\n",
        "def create_matrix(tokenized, vocab):\n",
        "    matrix = np.zeros((1, len(vocab)), dtype=int)\n",
        "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    word_counts = Counter(tokenized)\n",
        "    for word, count in word_counts.items():\n",
        "        if word in vocab_index:\n",
        "            word_idx = vocab_index[word]\n",
        "            matrix[0, word_idx] = count\n",
        "\n",
        "    return matrix\n",
        "\n",
        "matrix = create_matrix(tokenized, vocab)\n",
        "\n",
        "print(\"Bag of words matrix:\\n\", matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDbRJqoSDI6H",
        "outputId": "e10146d1-ce9e-4d0c-e768-e2407754ce75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of words matrix:\n",
            " [[ 7  2  5  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "   1  1  1  1  1  1  1  1  1  1  1  1  1 12  2  2  3  3  1  1  1  1  6  2\n",
            "   1  1  1  1  1  1  1  2  1  5  1  3  1  1  1  1  2  1  1  9  4  6  2  2\n",
            "   1  1  1  1  2  1  2  1  2  3  6  1  1  1  1  4  2  1  1  1  1  3  1  3\n",
            "   1  1  3  3  1  1  1  1  1  2  1  2  1  2  1  1  1  1  2  8  2  1  1  1\n",
            "   1  2  6  4  1  2  1  6  3  1  1  4  1  1  1  2  1  1  2  1  1  2  3  6\n",
            "   1  1  1  1  1  2  1  2  1  1  1  2  1  1  1  1  2  2  1  2  1  2  1  1\n",
            "   3  1  2  2  1  3  1  1  3  1  4  2  1  1  2  1  4  2  1  2  1  2  1  4\n",
            "   1  1  2  1  1  5  1  4  2  1  3  1  2  1  1  5  1  2  2  2  1  2  2  1\n",
            "   1  6  1  4  5  1  1  1  1  2  2  2  1  2  3  1  1  2  1  1  1  1  2  1\n",
            "   1  1  1 13  2  9  2  1  1  2  1  2  1  1  1  1  2  2  1  2  1  1  1  1\n",
            "   1  2  2  2  1  3  1  1  1  2  1  1  1  1  5  6  1  2  1  1  1  1  1  1\n",
            "   1  1  1  2  6  1  2  1  3  1  1  1  2  1  7  4  1  1  1  8  1  3  1  3\n",
            "   1  3  1  2  1  1  1  1  3  2  1  2  3  1  4  1  2  1  1  1  1  1  3  3\n",
            "   4  1  1  6  1  2  1  1  4  1 15  3  3  1  1  1  1  1  1  3  1  4  2  1\n",
            "   1  1  4 15  1  1  1  1  2  2  1  2  2  1  2  2  1  1  3  1  1 18  2  2\n",
            "   1  1  1  1  4  1  1  1  4  2  1  1  1  1  1  1  1  1  1  1  1  2  2  2\n",
            "   2  2  1  1  1  1  2  1  1  1  1  1  3  1  3  2  1  1  1  1  1  1  3  3\n",
            "   1  1  2  1  2  1  1  2  1  1  3  2  1  5  7  1  1  1  2  1  1  1  1  2\n",
            "   1  1  2  1  2  1  1  2  1  3  1  3  1  1  1  1  1  1  1  1  1  1 19 11\n",
            "   1  2  1  2  1  3  4  9 18  1  1  1  1  1  1  1  7  1  1  2  3  1 11  1\n",
            "   1  2  1  2  1  1  2  1  4  2  5  2  2  2  2  1  3  1  1  1  1  1  2  1\n",
            "   3  1  1  3  1  2  3  1  1  2  1  1  8  1  2 11  2  1  1  1  6 37  3 26\n",
            "   1  6  1  2  2  1  5  1  1  1  1  1  1  2  7  1  1 11  1  1  3  1  3  4\n",
            "   3  1 13  2  8  5 16  4  1  1  1  1  1  1  1  4  3 10  1  2  1  1  5  1\n",
            "   1  6  1  2  1  1  1  1  2  1  1  1  1  2  1  1  1  1  2  4  1  1  1  1\n",
            "   1  1  1  1  4  1  1  1  3  1  2  2  1  9  1  2  2  1  2 13 28  1  1  1\n",
            "   1  1  1  1  1  1  1  1  1  1  1  1  1  3  1  1  1  2  1  1  1  1  1  1\n",
            "   1  3  2  3  5  7  1  1  7  5  1  2  1  1  1  1  3  2  3  1  3  2  7  1\n",
            "   1  1  1  1  2  2  1  4  1  1  2  1  1  9  1  2  2  4  3  1  2  1  2  1\n",
            "   2  2  1  1  1  1  2  7  5 42  1  1  1  1  1  4  1  1  2  1  7  1  1  1\n",
            "   1  3  1  2  3  2  3  4 11  1  1  2  1  1  1  1  1  1  1  2  2  1  1  1\n",
            "   4  2  1  5  1  1  1  6  1  2  1  1  1 14  1  1  1  1  1  8 31  1  2  1\n",
            "   1  2  2 22  1  2  5  1  3  1  2  4  2  3  1  2  3  1  1  3  2  1  1  1\n",
            "   6  1 19  1  1  2  1  3  1  1  2  2  2  1  1  3  1 13  1  1  1 13  1  3\n",
            "   2  1  4  1  2  1  1  4  4  1  2  3  6  5  1  5  1  6  7  1  3  9 44  1\n",
            "   1  1  1  6  1  2  3  1  2  1  1  2  2  2  1  1  2  1  1  3  1  1  1  1\n",
            "   1  1  2  1  2  1  1  1 18 25 26  1  1  1  1  1  5  1  1  6  1  4  2  3\n",
            "   5  1  1  1  1  1  1  1  1  1  1  1  1  1  4  1  1  2  1  1  2 44  3  1\n",
            "   9  1  2  2  1  6  1  1  1  3  4  1  1  2  4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shrink bag of words. Remove words that are met once or twice \\\\\n",
        "I have a thought about leaving about 500 words, as it was mentioned in lab, but not in assignment. However i decided to stick to about 400, as it appeared after setting >= 2 and is close to initial thought of 500\n"
      ],
      "metadata": {
        "id": "u_vond80u11T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before:\")\n",
        "shrink_tokenized = text.copy()\n",
        "print(len(shrink_tokenized))\n",
        "word_counts = Counter(shrink_tokenized)\n",
        "print(len(word_counts))\n",
        "\n",
        "filtered_words = [word for word in shrink_tokenized if word_counts[word] >= 2]\n",
        "print(\"After:\")\n",
        "print(len(filtered_words))\n",
        "vocab = sorted(set(filtered_words))\n",
        "print(len(vocab))\n",
        "matrix = create_matrix(filtered_words, vocab)\n",
        "\n",
        "print(\"Bag of Words matrix:\\n\", matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nigLQG6YvC7s",
        "outputId": "b9f10540-59a1-4c53-c24f-1d91c5ca5e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "2308\n",
            "951\n",
            "After:\n",
            "1738\n",
            "381\n",
            "Bag of Words matrix:\n",
            " [[ 7  2  5  2 12  2  2  3  3  6  2  2  5  3  2  9  4  6  2  2  2  2  2  3\n",
            "   6  4  2  3  3  3  3  2  2  2  2  8  2  2  6  4  2  6  3  4  2  2  2  3\n",
            "   6  2  2  2  2  2  2  2  3  2  2  3  3  4  2  2  4  2  2  2  4  2  5  4\n",
            "   2  3  2  5  2  2  2  2  2  6  4  5  2  2  2  2  3  2  2 13  2  9  2  2\n",
            "   2  2  2  2  2  2  2  3  2  5  6  2  2  6  2  3  2  7  4  8  3  3  3  2\n",
            "   3  2  2  3  4  2  3  3  4  6  2  4 15  3  3  3  4  2  4 15  2  2  2  2\n",
            "   2  2  3 18  2  2  4  4  2  2  2  2  2  2  2  3  3  2  3  3  2  2  2  3\n",
            "   2  5  7  2  2  2  2  2  3  3 19 11  2  2  3  4  9 18  7  2  3 11  2  2\n",
            "   2  4  2  5  2  2  2  2  3  2  3  3  2  3  2  8  2 11  2  6 37  3 26  6\n",
            "   2  2  5  2  7 11  3  3  4  3 13  2  8  5 16  4  4  3 10  2  5  6  2  2\n",
            "   2  2  4  4  3  2  2  9  2  2  2 13 28  3  2  3  2  3  5  7  7  5  2  3\n",
            "   2  3  3  2  7  2  2  4  2  9  2  2  4  3  2  2  2  2  2  7  5 42  4  2\n",
            "   7  3  2  3  2  3  4 11  2  2  2  4  2  5  6  2 14  8 31  2  2  2 22  2\n",
            "   5  3  2  4  2  3  2  3  3  2  6 19  2  3  2  2  2  3 13 13  3  2  4  2\n",
            "   4  4  2  3  6  5  5  6  7  3  9 44  6  2  3  2  2  2  2  2  3  2  2 18\n",
            "  25 26  5  6  4  2  3  5  4  2  2 44  3  9  2  2  6  3  4  2  4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Generate vectors for documents and calculate cosine similarity"
      ],
      "metadata": {
        "id": "GYuTKwbjJFJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see things more clear in one cell I copied the functions from previous steps \\\\\n",
        "Without removing stop-words similarity is 0.8975659906121413, after it is 0.3597398988111578. Bag of words is not shrinked"
      ],
      "metadata": {
        "id": "hJiMkVdJKTha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def tokenize(text):\n",
        "    pattern = r\"[^\\w-]+\"\n",
        "    text = re.split(pattern, text)\n",
        "    text = [word for word in text if word]\n",
        "    text = [word for word in text if not re.match(r'^\\d+$', word)]\n",
        "    text = [word.lower() for word in text]\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    return text\n",
        "\n",
        "def count_words(words):\n",
        "    word_count = Counter(words)\n",
        "    return word_count\n",
        "\n",
        "def create_vocabulary(bow1, bow2):\n",
        "    vocabulary = set(bow1.keys()).union(set(bow2.keys()))\n",
        "    return list(vocabulary)\n",
        "\n",
        "def create_vector(bow, vocabulary):\n",
        "    vector = np.zeros(len(vocabulary))\n",
        "    word_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "    for word, count in bow.items():\n",
        "        if word in word_index:\n",
        "            vector[word_index[word]] = count\n",
        "    return vector\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    texts = []\n",
        "    flags = 0 | pymupdf.TEXT_DEHYPHENATE | pymupdf.TEXT_CID_FOR_UNKNOWN_UNICODE\n",
        "    with pymupdf.open(pdf_path) as doc:\n",
        "      for page in doc:\n",
        "          text = page.get_text(\"text\", flags=flags)\n",
        "          texts.append(text)\n",
        "      result_text = chr(12).join(texts)\n",
        "\n",
        "    return result_text\n",
        "\n",
        "\n",
        "pdf_path = \"lab1.pdf\"\n",
        "text1 = extract_text_from_pdf(pdf_path)\n",
        "pdf_path = \"lab1_2.pdf\"\n",
        "text2 = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "words1 = tokenize(text1)\n",
        "words2 = tokenize(text2)\n",
        "\n",
        "cw1 = count_words(words1)\n",
        "cw2 = count_words(words2)\n",
        "\n",
        "vocabulary = create_vocabulary(cw1, cw2)\n",
        "\n",
        "vector1 = create_vector(cw1, vocabulary)\n",
        "vector2 = create_vector(cw2, vocabulary)\n",
        "\n",
        "similarity = cosine_similarity(vector1, vector2)\n",
        "\n",
        "print(\"Cosine Similarity:\", similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQob1oYky0XL",
        "outputId": "b302d5bb-81c4-4809-ea46-4426765b1232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.3597398988111578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. Interpret results"
      ],
      "metadata": {
        "id": "BR8tPkkAUOSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 0.3597 similarity can be interpreted in a way that articles are not highly similar, but have some common words. Since articles cover different topics, I can suppose that low cosine similarity is the expected result. I think similarity is based on some common words like \"model\", \"neural\", etc., while the other significant part of the article covers other specifics"
      ],
      "metadata": {
        "id": "wlOixBRTUm-B"
      }
    }
  ]
}